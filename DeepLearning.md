
# Deeep learning

**内部表現**: 観測したデータ(事実)から本質的な情報を抽出したもの  

### パターン認識  
入力から特徴ベクトルを抽出しベクトルを識別気にかけてクラス分けする。  
ベクトルを抽出するときどれだけ少数の変数で抽出できるかが重要->**次元削減**  
観測データに潜在的な構造があると次元削減がやりやすくなる  

##### 例
```
# input
- みかん
- いちご
- メロン
- すいか
- りんご

# 特徴
[ 野菜:0 フルーツX:1, 赤:0 橙:1 緑:2, 仁果類:0 核果類:1 殻果類:2 柑橘類:3 その他(野菜):4 ]

# 特徴ベクレル化
- みかん -> [ 1, 1, 3]
- いちご -> [ 0, 0, 4]
- メロン -> [ 0, 2, 4]
- すいか -> [ 0, 2, 4]
- りんご -> [ 1, 0, 0]
```


**醜いあひるの子の定理**:  
> 醜いアヒルの子と普通のアヒルの子、すなわち、白鳥の子とアヒルの子とは、   
> 似通った2羽のアヒルの子が似ているのと同じ程度に似ている  
  
**ノーフリーランチ定理**: 
> コスト関数の極値を探索するあらゆるアルゴリズムは、  
> 全ての可能なコスト関数に適用した結果を平均すると同じ性能となる
  
**全てのタスクに対して優れている万能なアルゴリズムは存在しない**  
  
**表現学習**:  
人間があらかじめ出力結果を用意し、その中から結果を選択する方式。  
結果に対して客観的な評価尺度が必要

**主成分分析**:
次元削減後に分散が最大になるように軸を決める方法  
-> 軸は減るが違いがわかりやすくなる  
-> 元データが多変数正規分布ならば分散と情報量は比例する   
  
**因子分析**:  
主成分分析した後に軸を回転させる  
-> よりわかりやすい特徴を見つける  
  
**独立成分分析**:  
独立成分を抽出することで混合された情報から元の情報を見つける
   
  
## ニューラルネットワーク
脳神経系を真似した  

#### 脳神経系
**ニューロン**:
> 神経細胞というのは情報処理に特化したヒトなどの多細胞動物特有の細胞です。  
> 入力刺激を処理し、活動電位を発生させ、他の細胞に情報を伝達するという機能を持っています  
[引用元](http://www-user.yokohama-cu.ac.jp/~pharmac/04goshima/4goshima_neuron.html)  
ニューロンは３種類ある
- 感覚ニューロン: 受容器(皮膚等)から得た信号を中枢へ伝える
- 介在ニューロン: ニューロン同士の接続をする
- 運動ニューロン: 中枢からの信号を効果器(筋肉等)へ伝える
  
**シナプス**:  
> シナプスとは簡単に言えば、隣のニューロンに情報を伝える部分のことです  
[引用元](http://www-user.yokohama-cu.ac.jp/~pharmac/04goshima/4goshima_neuron.html)  
  
**パーセプトロン**  
脳の認知機能(ニューロンやシナプス)を模したパターン認識のモデル  
  
### ニューラルネットワーク
入力  
**X = (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>)**  
を受け取りyを出力するモデルの例
```
y = a( sum i = 1 to n wi * xi - θ )
a(x) = x < 0 ? 0 : 1
```
wi:i番目の入力要素xiに対する重み  
θ:閾値  
y = a(W<sup>T</sup>X)
とも書ける

**相互結合ニューラルネットワーク**   
ノード同士が相互に結合しているモデル
**階層型ニューラルネットワーク**  
ノードがそれぞれの層に分けられているモデル
- 入力層  
- 中間層  
- 出力層     
に分けられる

あるノードに与える信号を作るノードの集合を**受容野**と呼ぶ  

**教師あり学習**:  
入力Xと正解出力Tのペアの集合を学習用データとして重みWを修正していく  

**教師なし学習**:  
入力Xのみから学習を行う
  
**線形分離**:
入力信号を超平面で二分し、片方を１、もう片方を０とする処理
  
### 誤り訂正学習即Algorithm  
```python
# input
X = [
  [ 1, 1, 3],
  [ 0, 0, 4],
  [ 0, 2, 4],
  [ 0, 2, 4],
  [ 1, 0, 0]
]
W = [ 1, -1, -1]
theta = 0
Eta = 0.5
# Algorithm
def a(x):
  if x >= 0:
    return 1
  else:
    return 0
    
for i in X.size():
    y = 0
    for j in W.size():
      y += W[j]*X[i][j] - theta

    if a(y) != T[i]:
      for j in W.size():
        W[j] = W[j] + Eta*( T[j] - y )*X[j]

```
  
**オンライン学習**:
入力が与えられるたびに微修正していく学習方式

**バッチ処理**:  
結果を保持しておき、後ほどまとめて修正を行う

## 誤差逆伝播学習
あるノードに対して受容器の重みを修正する方法
うえの閾値関数aを連続なものにすることによって  
いろいろやりやすい
```
a(x) = sig(x) = 1 /(1+pow(e,-x)
```

確率的勾配降下法  
```
for i in range(N-1):
  for j in X.size():
    Y = []
    for k in W[i].size():
      Y[i][k] += W[i][k]*X[j][k] - theta
    
    if a(y) != T[j]:
      for k in W[i].size():
        W[i+1][k] = W[i+1][k] + Eta*R(W)/W[i+1][k]
```
ここでRが二乗誤差である時
```  
W[i+1][k] = W[i+1][k] + Eta*Delta[i+1]*Y[i][k]
```
となる。
